#!/usr/bin/env python3
"""
Smart database update - checks for new measures and updates only if needed
"""
import sys
import logging
import argparse
from pathlib import Path
from datetime import datetime

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.database.operations import Database
from src.database.deduplication import Deduplicator
from src.scrapers.ca_sos import CASOSScraper
from src.enrichment.summaries import SummaryGenerator
from src.config import LOG_LEVEL

# Set up logging
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def normalize_measure_data(data: dict) -> dict:
    """Normalize field names to match BallotMeasure model"""
    # Create a copy to avoid modifying original
    normalized = data.copy()
    
    # Field name mappings
    if 'source' in normalized:
        normalized['data_source'] = normalized.pop('source')
    
    if 'measure_text' in normalized and 'title' not in normalized:
        normalized['title'] = normalized.pop('measure_text')
    
    # Ensure required fields exist (will be generated by model)
    normalized.setdefault('fingerprint', '')
    normalized.setdefault('measure_fingerprint', '')
    normalized.setdefault('content_hash', '')
    
    # List of valid BallotMeasure fields
    valid_fields = {
        'fingerprint', 'measure_fingerprint', 'content_hash',
        'measure_id', 'measure_letter', 'year', 'state', 'county', 'jurisdiction',
        'title', 'description', 'ballot_question',
        'yes_votes', 'no_votes', 'total_votes', 'percent_yes', 'percent_no',
        'passed', 'pass_fail',
        'measure_type', 'topic_primary', 'topic_secondary', 'category_type', 'category_topic',
        'data_source', 'source_url', 'pdf_url',
        'has_summary', 'summary_title', 'summary_text',
        'election_type', 'election_date', 'decade', 'century',
        'created_at', 'updated_at', 'last_seen_at', 'update_count',
        'is_active', 'is_duplicate', 'duplicate_type', 'master_id', 'merged_from'
    }
    
    # Remove any fields not in the model
    filtered = {k: v for k, v in normalized.items() if k in valid_fields}
    
    return filtered


def check_for_updates(db: Database) -> dict:
    """Check for new measures without updating"""
    logger.info("Checking for new measures...")
    
    # Run scraper
    scraper = CASOSScraper()
    result = scraper.run(save_raw=False)
    
    new_count = 0
    update_count = 0
    
    for measure_data in result.get('measures', []):
        # Normalize the data before creating BallotMeasure
        normalized_data = normalize_measure_data(measure_data)
        
        from src.database.models import BallotMeasure
        measure = BallotMeasure(**normalized_data)
        
        # Check if exists
        existing = db.find_by_fingerprint(measure.fingerprint)
        if not existing:
            # Check content match
            content_matches = db.find_by_content_hash(measure.content_hash)
            if not content_matches:
                new_count += 1
            else:
                # Would be marked as duplicate
                pass
        else:
            # Check if would be updated
            if (not existing.has_summary and measure.has_summary) or \
               (not existing.pdf_url and measure.pdf_url):
                update_count += 1
    
    return {
        'total_scraped': len(result.get('measures', [])),
        'new': new_count,
        'updates': update_count,
        'measures': result.get('measures', [])
    }


def update_database(db: Database, measures: list) -> dict:
    """Update database with new measures"""
    logger.info(f"Processing {len(measures)} measures...")
    
    run_id = db.log_scraper_run('update')
    
    inserted = 0
    updated = 0
    duplicates = 0
    errors = 0
    
    deduplicator = Deduplicator(db)
    
    for measure_data in measures:
        try:
            # Normalize the data before creating BallotMeasure
            normalized_data = normalize_measure_data(measure_data)
            
            from src.database.models import BallotMeasure
            measure = BallotMeasure(**normalized_data)
            
            # Check for duplicates
            duplicate = deduplicator.check_duplicate(measure)
            
            if duplicate:
                if duplicate['type'] == 'exact':
                    # Update existing
                    if db.update_measure(duplicate['id'], normalized_data):
                        updated += 1
                else:
                    # Mark as duplicate
                    measure_id = db.insert_measure(measure)
                    deduplicator.mark_duplicate(
                        measure_id, 
                        duplicate['id'], 
                        duplicate['type']
                    )
                    duplicates += 1
            else:
                # Insert new
                db.insert_measure(measure)
                inserted += 1
                
        except Exception as e:
            errors += 1
            logger.error(f"Error processing measure: {e}")
    
    # Update scraper run
    db.update_scraper_run(
        run_id,
        measures_checked=len(measures),
        new_measures=inserted,
        updated_measures=updated,
        duplicates_found=duplicates,
        status='success' if errors == 0 else 'completed_with_errors'
    )
    
    return {
        'inserted': inserted,
        'updated': updated,
        'duplicates': duplicates,
        'errors': errors
    }


def main():
    """Main entry point for smart database updates"""
    parser = argparse.ArgumentParser(
        description='Smart database update for ballot measures'
    )
    parser.add_argument(
        '--check-only',
        action='store_true',
        help='Only check for new measures, do not update'
    )
    parser.add_argument(
        '--dedupe',
        action='store_true',
        help='Run cross-source deduplication after update'
    )
    parser.add_argument(
        '--enrich',
        action='store_true',
        help='Enrich with summaries after update'
    )
    parser.add_argument(
        '--stats',
        action='store_true',
        help='Show database statistics'
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='Force update even if no new measures'
    )
    
    args = parser.parse_args()
    
    # Initialize database
    with Database() as db:
        
        # Show statistics if requested
        if args.stats:
            stats = db.get_statistics()
            logger.info("\nDatabase Statistics:")
            logger.info(f"  Total measures: {stats['total_measures']:,}")
            logger.info(f"  With summaries: {stats['with_summaries']:,}")
            logger.info(f"  With vote data: {stats['with_votes']:,}")
            logger.info(f"  Year range: {stats['year_min']}-{stats['year_max']}")
            logger.info("\nBy source:")
            for source, count in stats['by_source'].items():
                logger.info(f"  {source}: {count:,}")
            return 0
        
        # Check for updates
        check_result = check_for_updates(db)
        
        logger.info(f"\nCheck Results:")
        logger.info(f"  Scraped: {check_result['total_scraped']} measures")
        logger.info(f"  New: {check_result['new']} measures")
        logger.info(f"  Updates available: {check_result['updates']} measures")
        
        # If check-only, stop here
        if args.check_only:
            if check_result['new'] > 0:
                logger.info("\n✨ New measures found! Run without --check-only to update.")
            else:
                logger.info("\n✅ No new measures found. Database is up to date.")
            return 0
        
        # Update if needed or forced
        if check_result['new'] > 0 or check_result['updates'] > 0 or args.force:
            logger.info("\nUpdating database...")
            update_result = update_database(db, check_result['measures'])
            
            logger.info(f"\nUpdate Results:")
            logger.info(f"  Inserted: {update_result['inserted']}")
            logger.info(f"  Updated: {update_result['updated']}")
            logger.info(f"  Duplicates: {update_result['duplicates']}")
            logger.info(f"  Errors: {update_result['errors']}")
            
            # Run deduplication if requested
            if args.dedupe:
                logger.info("\nRunning cross-source deduplication...")
                deduplicator = Deduplicator(db)
                deduplicator.deduplicate_cross_source()
                
                # Show deduplication report
                report = deduplicator.get_duplicate_report()
                logger.info(f"\nDeduplication Report:")
                logger.info(f"  Total duplicates: {report['total_duplicates']}")
                logger.info(f"  Cross-source groups: {report['cross_source_groups']}")
            
            # Enrich with summaries if requested
            if args.enrich:
                logger.info("\nEnriching with summaries...")
                generator = SummaryGenerator(db)
                generator.enrich_measures(limit=5)
        else:
            logger.info("\n✅ No updates needed. Database is current.")
    
    return 0


if __name__ == '__main__':
    sys.exit(main())