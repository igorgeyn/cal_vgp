#!/usr/bin/env python3
"""
Entry point for scraping ballot measures
"""
import sys
import logging
import argparse
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.scrapers.ca_sos import CASOSScraper, UCLawSFScraper
from src.parsers.ceda import CEDAParser
from src.database.operations import Database
from src.enrichment.summaries import SummaryGenerator
from src.config import LOG_LEVEL

# Set up logging
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def normalize_measure_data(data: dict) -> dict:
    """Normalize field names to match BallotMeasure model"""
    normalized = data.copy()
    
    # Field name mappings
    if 'source' in normalized:
        normalized['data_source'] = normalized.pop('source')
    
    if 'measure_text' in normalized and 'title' not in normalized:
        normalized['title'] = normalized.pop('measure_text')
    
    # Ensure required fields exist (will be generated by model)
    normalized.setdefault('fingerprint', '')
    normalized.setdefault('measure_fingerprint', '')
    normalized.setdefault('content_hash', '')
    
    # List of valid BallotMeasure fields
    valid_fields = {
        'fingerprint', 'measure_fingerprint', 'content_hash',
        'measure_id', 'measure_letter', 'year', 'state', 'county', 'jurisdiction',
        'title', 'description', 'ballot_question',
        'yes_votes', 'no_votes', 'total_votes', 'percent_yes', 'percent_no',
        'passed', 'pass_fail',
        'measure_type', 'topic_primary', 'topic_secondary', 'category_type', 'category_topic',
        'data_source', 'source_url', 'pdf_url',
        'has_summary', 'summary_title', 'summary_text',
        'election_type', 'election_date', 'decade', 'century',
        'created_at', 'updated_at', 'last_seen_at', 'update_count',
        'is_active', 'is_duplicate', 'duplicate_type', 'master_id', 'merged_from'
    }
    
    # Remove any fields not in the model
    filtered = {k: v for k, v in normalized.items() if k in valid_fields}
    
    return filtered


def scrape_ca_sos():
    """Scrape California Secretary of State"""
    logger.info("Starting CA SOS scrape...")
    scraper = CASOSScraper()
    return scraper.run()


def scrape_uc_law_sf(max_items=50):
    """Scrape UC Law SF historical data"""
    logger.info(f"Starting UC Law SF scrape (max {max_items} items)...")
    scraper = UCLawSFScraper(max_items=max_items)
    return scraper.run()


def parse_ceda():
    """Parse CEDA historical data files"""
    logger.info("Starting CEDA data parsing...")
    parser = CEDAParser()
    measures = parser.parse_all_files()
    
    if measures:
        parser.save_parsed_data(measures)
        
    return {
        'source': 'CEDA',
        'total_measures': len(measures),
        'measures': [m.to_dict() for m in measures]
    }


def main():
    """Main scraping entry point"""
    parser = argparse.ArgumentParser(description='Scrape California ballot measures')
    parser.add_argument(
        '--source',
        choices=['all', 'ca-sos', 'uc-law-sf', 'ceda'],
        default='all',
        help='Data source to scrape'
    )
    parser.add_argument(
        '--max-historical',
        type=int,
        default=50,
        help='Maximum historical items to scrape from UC Law SF'
    )
    parser.add_argument(
        '--no-save',
        action='store_true',
        help='Do not save to database'
    )
    parser.add_argument(
        '--enrich',
        action='store_true',
        help='Enrich with summaries after scraping'
    )
    
    args = parser.parse_args()
    
    results = []
    
    # Run scrapers based on source
    if args.source in ['all', 'ca-sos']:
        try:
            results.append(scrape_ca_sos())
        except Exception as e:
            logger.error(f"CA SOS scrape failed: {e}")
            
    if args.source in ['all', 'uc-law-sf']:
        try:
            results.append(scrape_uc_law_sf(args.max_historical))
        except Exception as e:
            logger.error(f"UC Law SF scrape failed: {e}")
            
    if args.source in ['all', 'ceda']:
        try:
            results.append(parse_ceda())
        except Exception as e:
            logger.error(f"CEDA parse failed: {e}")
    
    # Display results
    total_measures = sum(r['total_measures'] for r in results if r)
    logger.info(f"\nScraping complete!")
    logger.info(f"Total measures collected: {total_measures}")
    
    for result in results:
        if result:
            logger.info(f"  {result['source']}: {result['total_measures']} measures")
    
    # Save to database if requested
    if not args.no_save and results:
        logger.info("\nSaving to database...")
        
        with Database() as db:
            run_id = db.log_scraper_run('scrape')
            
            inserted = 0
            updated = 0
            errors = 0
            
            for result in results:
                for measure_data in result.get('measures', []):
                    try:
                        # Normalize the data before creating BallotMeasure
                        normalized_data = normalize_measure_data(measure_data)
                        
                        from src.database.models import BallotMeasure
                        measure = BallotMeasure(**normalized_data)
                        
                        # Try to insert
                        try:
                            measure_id = db.insert_measure(measure)
                            inserted += 1
                        except Exception as e:
                            if "already exists" in str(e):
                                # Try to update instead
                                existing = db.find_by_fingerprint(measure.fingerprint)
                                if existing:
                                    db.update_measure(existing.id, normalized_data)
                                    updated += 1
                            else:
                                errors += 1
                                logger.error(f"Error saving measure: {e}")
                                
                    except Exception as e:
                        errors += 1
                        logger.error(f"Error processing measure: {e}")
            
            # Update scraper run
            db.update_scraper_run(
                run_id,
                measures_checked=total_measures,
                new_measures=inserted,
                updated_measures=updated,
                status='success' if errors == 0 else 'completed_with_errors'
            )
            
            logger.info(f"Database update complete:")
            logger.info(f"  Inserted: {inserted}")
            logger.info(f"  Updated: {updated}")
            logger.info(f"  Errors: {errors}")
    
    # Enrich with summaries if requested
    if args.enrich:
        logger.info("\nEnriching with summaries...")
        generator = SummaryGenerator()
        generator.enrich_measures(limit=10)
        
        stats = generator.get_summary_statistics()
        logger.info(f"Summary coverage: {stats['total_with_summaries']} measures")
    
    return 0  # Always return success


if __name__ == '__main__':
    sys.exit(main())